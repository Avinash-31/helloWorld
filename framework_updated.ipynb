{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment and Dependencies\n",
    "Install necessary libraries such as PySpark, Snowflake connector, and Iceberg connector. Configure the Spark environment with the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install pyspark\n",
    "!pip install snowflake-connector-python\n",
    "!pip install iceberg\n",
    "\n",
    "# Configure the Spark environment with the required dependencies\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Snowflake and Iceberg connectors\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataReplicationFramework\") \\\n",
    "    .config(\"spark.jars.packages\", \"net.snowflake:snowflake-jdbc,org.apache.iceberg:iceberg-spark-runtime-3.2_2.12\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Spark session\n",
    "spark.sql(\"SELECT 'Spark session initialized' AS status\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Configuration Parameters\n",
    "Define all configuration parameters in a single cell, including source and target table names, database credentials, processing modes, and logging levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Configuration Parameters\n",
    "\n",
    "# Configuration parameters for source and target tables\n",
    "source_table = \"source_table_name\"\n",
    "target_table = \"target_table_name\"\n",
    "\n",
    "# Snowflake connection parameters\n",
    "snowflake_options = {\n",
    "    \"sfURL\": \"your_snowflake_account_url\",\n",
    "    \"sfUser\": \"your_snowflake_username\",\n",
    "    \"sfPassword\": \"your_snowflake_password\",\n",
    "    \"sfDatabase\": \"your_snowflake_database\",\n",
    "    \"sfSchema\": \"your_snowflake_schema\",\n",
    "    \"sfWarehouse\": \"your_snowflake_warehouse\"\n",
    "}\n",
    "\n",
    "# Iceberg connection parameters\n",
    "iceberg_catalog = \"your_iceberg_catalog\"\n",
    "iceberg_namespace = \"your_iceberg_namespace\"\n",
    "\n",
    "# Processing modes\n",
    "processing_mode = \"full_load\"  # Options: full_load, chunk_load, partition_load, bucketing, incremental_load\n",
    "\n",
    "# Logging levels\n",
    "logging_level = \"INFO\"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "\n",
    "# Control table name\n",
    "control_table = \"control_table_name\"\n",
    "\n",
    "# Spark session configuration for Snowflake\n",
    "spark.conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\n",
    "spark.conf.set(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://localhost:9083\")\n",
    "\n",
    "# Function to log messages\n",
    "def log_message(level, message):\n",
    "    if level == \"DEBUG\":\n",
    "        print(f\"DEBUG: {message}\")\n",
    "    elif level == \"INFO\":\n",
    "        print(f\"INFO: {message}\")\n",
    "    elif level == \"WARNING\":\n",
    "        print(f\"WARNING: {message}\")\n",
    "    elif level == \"ERROR\":\n",
    "        print(f\"ERROR: {message}\")\n",
    "    elif level == \"CRITICAL\":\n",
    "        print(f\"CRITICAL: {message}\")\n",
    "\n",
    "# Set logging level\n",
    "log_message(logging_level, \"Configuration parameters defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark Session\n",
    "Initialize a Spark session with the necessary configurations for connecting to Snowflake and Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Snowflake and Iceberg connectors\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataReplicationFramework\") \\\n",
    "    .config(\"spark.jars.packages\", \"net.snowflake:snowflake-jdbc,org.apache.iceberg:iceberg-spark-runtime-3.2_2.12\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://localhost:9083\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Spark session\n",
    "spark.sql(\"SELECT 'Spark session initialized' AS status\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utility Functions\n",
    "Define utility functions for logging, error handling, and schema conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Utility function for logging\n",
    "def log_message(level, message):\n",
    "    if level == \"DEBUG\":\n",
    "        print(f\"DEBUG: {message}\")\n",
    "    elif level == \"INFO\":\n",
    "        print(f\"INFO: {message}\")\n",
    "    elif level == \"WARNING\":\n",
    "        print(f\"WARNING: {message}\")\n",
    "    elif level == \"ERROR\":\n",
    "        print(f\"ERROR: {message}\")\n",
    "    elif level == \"CRITICAL\":\n",
    "        print(f\"CRITICAL: {message}\")\n",
    "\n",
    "# Utility function for error handling\n",
    "def handle_error(e):\n",
    "    log_message(\"ERROR\", str(e))\n",
    "    raise e\n",
    "\n",
    "# Utility function for schema conversion from Snowflake to Iceberg\n",
    "def convert_schema(snowflake_df: DataFrame) -> str:\n",
    "    try:\n",
    "        iceberg_schema = \", \".join([f\"{field.name} {field.dataType.simpleString()}\" for field in snowflake_df.schema.fields])\n",
    "        return iceberg_schema\n",
    "    except Exception as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Example usage of utility functions\n",
    "try:\n",
    "    # Example Snowflake query\n",
    "    snowflake_query = f\"SELECT * FROM {source_table} LIMIT 1\"\n",
    "    snowflake_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_options) \\\n",
    "        .option(\"query\", snowflake_query) \\\n",
    "        .load()\n",
    "    \n",
    "    # Convert schema\n",
    "    iceberg_schema = convert_schema(snowflake_df)\n",
    "    log_message(\"INFO\", f\"Converted schema: {iceberg_schema}\")\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Schema Handling\n",
    "Implement functions to fetch the schema from the Snowflake source table and convert it to a compatible Iceberg schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Schema Handling\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Utility function for logging\n",
    "def log_message(level, message):\n",
    "    if level == \"DEBUG\":\n",
    "        print(f\"DEBUG: {message}\")\n",
    "    elif level == \"INFO\":\n",
    "        print(f\"INFO: {message}\")\n",
    "    elif level == \"WARNING\":\n",
    "        print(f\"WARNING: {message}\")\n",
    "    elif level == \"ERROR\":\n",
    "        print(f\"ERROR: {message}\")\n",
    "    elif level == \"CRITICAL\":\n",
    "        print(f\"CRITICAL: {message}\")\n",
    "\n",
    "# Utility function for error handling\n",
    "def handle_error(e):\n",
    "    log_message(\"ERROR\", str(e))\n",
    "    raise e\n",
    "\n",
    "# Utility function for schema conversion from Snowflake to Iceberg\n",
    "def convert_schema(snowflake_df: DataFrame) -> str:\n",
    "    try:\n",
    "        iceberg_schema = \", \".join([f\"{field.name} {field.dataType.simpleString()}\" for field in snowflake_df.schema.fields])\n",
    "        return iceberg_schema\n",
    "    except Exception as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Example usage of utility functions\n",
    "try:\n",
    "    # Example Snowflake query\n",
    "    snowflake_query = f\"SELECT * FROM {source_table} LIMIT 1\"\n",
    "    snowflake_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_options) \\\n",
    "        .option(\"query\", snowflake_query) \\\n",
    "        .load()\n",
    "    \n",
    "    # Convert schema\n",
    "    iceberg_schema = convert_schema(snowflake_df)\n",
    "    log_message(\"INFO\", f\"Converted schema: {iceberg_schema}\")\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Control Table Management\n",
    "Implement functions to manage the control table, including inserting new table entries, updating batch sequence keys, and reverting updates on failure. Use SQL queries for control table management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Control Table Management\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Function to insert a new entry into the control table\n",
    "def insert_control_table_entry(table_name: str, batch_sk: int):\n",
    "    try:\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {control_table} (table_name, batch_sk)\n",
    "        VALUES ('{table_name}', {batch_sk})\n",
    "        \"\"\"\n",
    "        spark.sql(insert_query)\n",
    "        log_message(\"INFO\", f\"Inserted new entry into control table: {table_name}, batch_sk: {batch_sk}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Function to update the batch_sk in the control table\n",
    "def update_control_table_batch_sk(table_name: str, batch_sk: int):\n",
    "    try:\n",
    "        update_query = f\"\"\"\n",
    "        UPDATE {control_table}\n",
    "        SET batch_sk = {batch_sk}\n",
    "        WHERE table_name = '{table_name}'\n",
    "        \"\"\"\n",
    "        spark.sql(update_query)\n",
    "        log_message(\"INFO\", f\"Updated batch_sk in control table: {table_name}, batch_sk: {batch_sk}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Function to revert the batch_sk update in the control table\n",
    "def revert_control_table_batch_sk(table_name: str, previous_batch_sk: int):\n",
    "    try:\n",
    "        revert_query = f\"\"\"\n",
    "        UPDATE {control_table}\n",
    "        SET batch_sk = {previous_batch_sk}\n",
    "        WHERE table_name = '{table_name}'\n",
    "        \"\"\"\n",
    "        spark.sql(revert_query)\n",
    "        log_message(\"INFO\", f\"Reverted batch_sk in control table: {table_name}, batch_sk: {previous_batch_sk}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Example usage of control table management functions\n",
    "try:\n",
    "    # Insert a new entry\n",
    "    insert_control_table_entry(source_table, 0)\n",
    "    \n",
    "    # Update the batch_sk\n",
    "    update_control_table_batch_sk(source_table, 1)\n",
    "    \n",
    "    # Revert the batch_sk update\n",
    "    revert_control_table_batch_sk(source_table, 0)\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Data Loading Strategies\n",
    "Implement different data loading strategies such as full load, chunk load, partition load, and incremental load. Provide sample queries for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Data Loading Strategies\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Function to perform full load\n",
    "def full_load(source_table: str, target_table: str):\n",
    "    try:\n",
    "        # Load data from Snowflake\n",
    "        snowflake_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_options) \\\n",
    "            .option(\"dbtable\", source_table) \\\n",
    "            .load()\n",
    "        \n",
    "        # Write data to Iceberg\n",
    "        snowflake_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        \n",
    "        log_message(\"INFO\", f\"Full load completed for table: {source_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Function to perform chunk load\n",
    "def chunk_load(source_table: str, target_table: str, chunk_size: int):\n",
    "    try:\n",
    "        offset = 0\n",
    "        while True:\n",
    "            # Load chunk of data from Snowflake\n",
    "            snowflake_query = f\"SELECT * FROM {source_table} LIMIT {chunk_size} OFFSET {offset}\"\n",
    "            snowflake_df = spark.read \\\n",
    "                .format(\"snowflake\") \\\n",
    "                .options(**snowflake_options) \\\n",
    "                .option(\"query\", snowflake_query) \\\n",
    "                .load()\n",
    "            \n",
    "            if snowflake_df.count() == 0:\n",
    "                break\n",
    "            \n",
    "            # Write chunk of data to Iceberg\n",
    "            snowflake_df.write \\\n",
    "                .format(\"iceberg\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .save(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "            \n",
    "            offset += chunk_size\n",
    "            log_message(\"INFO\", f\"Chunk load completed for offset: {offset}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Function to perform partition load\n",
    "def partition_load(source_table: str, target_table: str, partition_column: str):\n",
    "    try:\n",
    "        # Load data from Snowflake\n",
    "        snowflake_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_options) \\\n",
    "            .option(\"dbtable\", source_table) \\\n",
    "            .load()\n",
    "        \n",
    "        # Write data to Iceberg with partitioning\n",
    "        snowflake_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(partition_column) \\\n",
    "            .save(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        \n",
    "        log_message(\"INFO\", f\"Partition load completed for table: {source_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Function to perform incremental load\n",
    "def incremental_load(source_table: str, target_table: str, batch_sk_column: str):\n",
    "    try:\n",
    "        # Get the max batch_sk from control table\n",
    "        max_batch_sk_query = f\"SELECT MAX(batch_sk) AS max_batch_sk FROM {control_table} WHERE table_name = '{source_table}'\"\n",
    "        max_batch_sk_df = spark.sql(max_batch_sk_query)\n",
    "        max_batch_sk = max_batch_sk_df.collect()[0][\"max_batch_sk\"]\n",
    "        \n",
    "        # Load new data from Snowflake\n",
    "        snowflake_query = f\"SELECT * FROM {source_table} WHERE {batch_sk_column} > {max_batch_sk}\"\n",
    "        snowflake_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_options) \\\n",
    "            .option(\"query\", snowflake_query) \\\n",
    "            .load()\n",
    "        \n",
    "        # Write new data to Iceberg\n",
    "        snowflake_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        \n",
    "        # Update control table with new max batch_sk\n",
    "        new_max_batch_sk = snowflake_df.agg({batch_sk_column: \"max\"}).collect()[0][0]\n",
    "        insert_control_table_entry(source_table, new_max_batch_sk)\n",
    "        \n",
    "        log_message(\"INFO\", f\"Incremental load completed for table: {source_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Example usage of data loading strategies\n",
    "try:\n",
    "    if processing_mode == \"full_load\":\n",
    "        full_load(source_table, target_table)\n",
    "    elif processing_mode == \"chunk_load\":\n",
    "        chunk_load(source_table, target_table, chunk_size=1000)\n",
    "    elif processing_mode == \"partition_load\":\n",
    "        partition_load(source_table, target_table, partition_column=\"partition_column_name\")\n",
    "    elif processing_mode == \"incremental_load\":\n",
    "        incremental_load(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Data Change Handling\n",
    "Implement logic to capture INSERT, UPDATE, and DELETE operations from Snowflake and reflect these changes in Iceberg efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Data Change Handling\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Function to capture INSERT operations from Snowflake and reflect in Iceberg\n",
    "def handle_insert_operations(source_table: str, target_table: str, batch_sk_column: str):\n",
    "    try:\n",
    "        # Load new data from Snowflake\n",
    "        snowflake_query = f\"SELECT * FROM {source_table} WHERE {batch_sk_column} > (SELECT MAX(batch_sk) FROM {control_table} WHERE table_name = '{source_table}')\"\n",
    "        snowflake_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_options) \\\n",
    "            .option(\"query\", snowflake_query) \\\n",
    "            .load()\n",
    "        \n",
    "        # Write new data to Iceberg\n",
    "        snowflake_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        \n",
    "        # Update control table with new max batch_sk\n",
    "        new_max_batch_sk = snowflake_df.agg({batch_sk_column: \"max\"}).collect()[0][0]\n",
    "        insert_control_table_entry(source_table, new_max_batch_sk)\n",
    "        \n",
    "        log_message(\"INFO\", f\"Insert operations handled for table: {source_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Function to capture UPDATE operations from Snowflake and reflect in Iceberg\n",
    "def handle_update_operations(source_table: str, target_table: str, batch_sk_column: str):\n",
    "    try:\n",
    "        # Load updated data from Snowflake\n",
    "        snowflake_query = f\"SELECT * FROM {source_table} WHERE {batch_sk_column} > (SELECT MAX(batch_sk) FROM {control_table} WHERE table_name = '{source_table}')\"\n",
    "        snowflake_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_options) \\\n",
    "            .option(\"query\", snowflake_query) \\\n",
    "            .load()\n",
    "        \n",
    "        # Write updated data to Iceberg\n",
    "        snowflake_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        \n",
    "        # Update control table with new max batch_sk\n",
    "        new_max_batch_sk = snowflake_df.agg({batch_sk_column: \"max\"}).collect()[0][0]\n",
    "        insert_control_table_entry(source_table, new_max_batch_sk)\n",
    "        \n",
    "        log_message(\"INFO\", f\"Update operations handled for table: {source_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Function to capture DELETE operations from Snowflake and reflect in Iceberg\n",
    "def handle_delete_operations(source_table: str, target_table: str, batch_sk_column: str):\n",
    "    try:\n",
    "        # Load deleted data from Snowflake\n",
    "        snowflake_query = f\"SELECT * FROM {source_table} WHERE {batch_sk_column} > (SELECT MAX(batch_sk) FROM {control_table} WHERE table_name = '{source_table}')\"\n",
    "        snowflake_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_options) \\\n",
    "            .option(\"query\", snowflake_query) \\\n",
    "            .load()\n",
    "        \n",
    "        # Get primary key column(s) for deletion\n",
    "        primary_key_columns = [field.name for field in snowflake_df.schema.fields if field.metadata.get(\"primary_key\", False)]\n",
    "        \n",
    "        # Delete data from Iceberg\n",
    "        for row in snowflake_df.collect():\n",
    "            delete_condition = \" AND \".join([f\"{col} = '{row[col]}'\" for col in primary_key_columns])\n",
    "            delete_query = f\"DELETE FROM {target_table} WHERE {delete_condition}\"\n",
    "            spark.sql(delete_query)\n",
    "        \n",
    "        # Update control table with new max batch_sk\n",
    "        new_max_batch_sk = snowflake_df.agg({batch_sk_column: \"max\"}).collect()[0][0]\n",
    "        insert_control_table_entry(source_table, new_max_batch_sk)\n",
    "        \n",
    "        log_message(\"INFO\", f\"Delete operations handled for table: {source_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Example usage of data change handling functions\n",
    "try:\n",
    "    handle_insert_operations(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "    handle_update_operations(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "    handle_delete_operations(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Merge Strategies\n",
    "Implement configurable merge strategies such as append, overwrite, and merge (upsert) for handling data changes in Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Merge Strategies\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Function to perform append operation\n",
    "def append_data(source_df: DataFrame, target_table: str):\n",
    "    try:\n",
    "        source_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        log_message(\"INFO\", f\"Append operation completed for table: {target_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Function to perform overwrite operation\n",
    "def overwrite_data(source_df: DataFrame, target_table: str):\n",
    "    try:\n",
    "        source_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        log_message(\"INFO\", f\"Overwrite operation completed for table: {target_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Function to perform merge (upsert) operation\n",
    "def merge_data(source_df: DataFrame, target_table: str, primary_key: str):\n",
    "    try:\n",
    "        target_df = spark.read \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .load(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        \n",
    "        # Perform merge (upsert) operation\n",
    "        merged_df = source_df.alias(\"source\").join(\n",
    "            target_df.alias(\"target\"),\n",
    "            source_df[primary_key] == target_df[primary_key],\n",
    "            \"outer\"\n",
    "        ).select(\n",
    "            *[f\"coalesce(source.{col}, target.{col}) as {col}\" for col in source_df.columns]\n",
    "        )\n",
    "        \n",
    "        merged_df.write \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        \n",
    "        log_message(\"INFO\", f\"Merge (upsert) operation completed for table: {target_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Example usage of merge strategies\n",
    "try:\n",
    "    # Load data from Snowflake\n",
    "    snowflake_query = f\"SELECT * FROM {source_table}\"\n",
    "    source_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_options) \\\n",
    "        .option(\"query\", snowflake_query) \\\n",
    "        .load()\n",
    "    \n",
    "    # Perform append operation\n",
    "    append_data(source_df, target_table)\n",
    "    \n",
    "    # Perform overwrite operation\n",
    "    overwrite_data(source_df, target_table)\n",
    "    \n",
    "    # Perform merge (upsert) operation\n",
    "    merge_data(source_df, target_table, primary_key=\"id\")\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Logging\n",
    "Implement logging at each step to track progress and potential errors. Use a logging framework to manage different logging levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging_level, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Utility function for logging\n",
    "def log_message(level, message):\n",
    "    if level == \"DEBUG\":\n",
    "        logger.debug(message)\n",
    "    elif level == \"INFO\":\n",
    "        logger.info(message)\n",
    "    elif level == \"WARNING\":\n",
    "        logger.warning(message)\n",
    "    elif level == \"ERROR\":\n",
    "        logger.error(message)\n",
    "    elif level == \"CRITICAL\":\n",
    "        logger.critical(message)\n",
    "\n",
    "# Example usage of logging\n",
    "log_message(\"INFO\", \"Logging is configured successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Main Replication Function\n",
    "Define the main replication function that orchestrates the entire data replication process, including schema handling, control table management, data loading, and merge strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Main Replication Function\n",
    "\n",
    "def replicate_data(source_table: str, target_table: str, batch_sk_column: str, processing_mode: str):\n",
    "    try:\n",
    "        # Step 1: Schema Handling\n",
    "        snowflake_query = f\"SELECT * FROM {source_table} LIMIT 1\"\n",
    "        snowflake_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_options) \\\n",
    "            .option(\"query\", snowflake_query) \\\n",
    "            .load()\n",
    "        \n",
    "        iceberg_schema = convert_schema(snowflake_df)\n",
    "        log_message(\"INFO\", f\"Schema converted: {iceberg_schema}\")\n",
    "        \n",
    "        # Step 2: Control Table Management\n",
    "        max_batch_sk_query = f\"SELECT MAX(batch_sk) AS max_batch_sk FROM {control_table} WHERE table_name = '{source_table}'\"\n",
    "        max_batch_sk_df = spark.sql(max_batch_sk_query)\n",
    "        max_batch_sk = max_batch_sk_df.collect()[0][\"max_batch_sk\"] or 0\n",
    "        \n",
    "        # Step 3: Data Loading\n",
    "        if processing_mode == \"full_load\":\n",
    "            full_load(source_table, target_table)\n",
    "        elif processing_mode == \"chunk_load\":\n",
    "            chunk_load(source_table, target_table, chunk_size=1000)\n",
    "        elif processing_mode == \"partition_load\":\n",
    "            partition_load(source_table, target_table, partition_column=\"partition_column_name\")\n",
    "        elif processing_mode == \"incremental_load\":\n",
    "            incremental_load(source_table, target_table, batch_sk_column)\n",
    "        \n",
    "        # Step 4: Merge Strategy\n",
    "        snowflake_query = f\"SELECT * FROM {source_table} WHERE {batch_sk_column} > {max_batch_sk}\"\n",
    "        source_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_options) \\\n",
    "            .option(\"query\", snowflake_query) \\\n",
    "            .load()\n",
    "        \n",
    "        merge_strategy = \"append\"  # Options: append, overwrite, merge\n",
    "        if merge_strategy == \"append\":\n",
    "            append_data(source_df, target_table)\n",
    "        elif merge_strategy == \"overwrite\":\n",
    "            overwrite_data(source_df, target_table)\n",
    "        elif merge_strategy == \"merge\":\n",
    "            merge_data(source_df, target_table, primary_key=\"id\")\n",
    "        \n",
    "        # Step 5: Data Change Handling\n",
    "        handle_insert_operations(source_table, target_table, batch_sk_column)\n",
    "        handle_update_operations(source_table, target_table, batch_sk_column)\n",
    "        handle_delete_operations(source_table, target_table, batch_sk_column)\n",
    "        \n",
    "        log_message(\"INFO\", f\"Data replication completed for table: {source_table}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Example usage of the main replication function\n",
    "try:\n",
    "    replicate_data(source_table, target_table, batch_sk_column=\"batch_sk\", processing_mode=processing_mode)\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Replication Framework\n",
    "Test the replication framework with different scenarios, including initial table creation, new data insertion, and data change handling. Verify the data in Iceberg against the source data in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Replication Framework\n",
    "\n",
    "# Test the replication framework with different scenarios, including initial table creation, new data insertion, and data change handling.\n",
    "# Verify the data in Iceberg against the source data in Snowflake.\n",
    "\n",
    "# Function to verify data in Iceberg against the source data in Snowflake\n",
    "def verify_data(source_table: str, target_table: str, batch_sk_column: str):\n",
    "    try:\n",
    "        # Load data from Snowflake\n",
    "        snowflake_query = f\"SELECT * FROM {source_table}\"\n",
    "        snowflake_df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_options) \\\n",
    "            .option(\"query\", snowflake_query) \\\n",
    "            .load()\n",
    "        \n",
    "        # Load data from Iceberg\n",
    "        iceberg_df = spark.read \\\n",
    "            .format(\"iceberg\") \\\n",
    "            .load(f\"{iceberg_catalog}.{iceberg_namespace}.{target_table}\")\n",
    "        \n",
    "        # Compare data\n",
    "        snowflake_count = snowflake_df.count()\n",
    "        iceberg_count = iceberg_df.count()\n",
    "        \n",
    "        if snowflake_count == iceberg_count:\n",
    "            log_message(\"INFO\", f\"Data verification successful for table: {source_table}\")\n",
    "        else:\n",
    "            log_message(\"ERROR\", f\"Data verification failed for table: {source_table}. Snowflake count: {snowflake_count}, Iceberg count: {iceberg_count}\")\n",
    "    except AnalysisException as e:\n",
    "        handle_error(e)\n",
    "\n",
    "# Test Case 1: Initial Table Creation & Data Insertion\n",
    "try:\n",
    "    # Replicate data\n",
    "    replicate_data(source_table, target_table, batch_sk_column=\"batch_sk\", processing_mode=\"full_load\")\n",
    "    \n",
    "    # Verify data\n",
    "    verify_data(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)\n",
    "\n",
    "# Test Case 2: Existing Table with New Data\n",
    "try:\n",
    "    # Insert new data into Snowflake (this is a placeholder, actual insertion code will depend on your setup)\n",
    "    # spark.sql(\"INSERT INTO source_table_name (columns) VALUES (values)\")\n",
    "    \n",
    "    # Replicate data\n",
    "    replicate_data(source_table, target_table, batch_sk_column=\"batch_sk\", processing_mode=\"incremental_load\")\n",
    "    \n",
    "    # Verify data\n",
    "    verify_data(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)\n",
    "\n",
    "# Test Case 3: Data Change Handling\n",
    "try:\n",
    "    # Handle data changes\n",
    "    handle_insert_operations(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "    handle_update_operations(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "    handle_delete_operations(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "    \n",
    "    # Verify data\n",
    "    verify_data(source_table, target_table, batch_sk_column=\"batch_sk\")\n",
    "except AnalysisException as e:\n",
    "    handle_error(e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
