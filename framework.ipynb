{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including PySpark, Snowflake connector, and Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import snowflake.connector\n",
    "import logging\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Snowflake to Iceberg Replication\") \\\n",
    "    .config(\"spark.jars.packages\", \"net.snowflake:snowflake-jdbc,net.snowflake:spark-snowflake\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Setup\n",
    "Define all configurations such as source & target tables, processing modes, and logging levels in a single cell for easy reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Setup\n",
    "\n",
    "# Snowflake configuration\n",
    "snowflake_config = {\n",
    "    \"sfURL\": \"your_snowflake_account_url\",\n",
    "    \"sfUser\": \"your_snowflake_username\",\n",
    "    \"sfPassword\": \"your_snowflake_password\",\n",
    "    \"sfDatabase\": \"your_snowflake_database\",\n",
    "    \"sfSchema\": \"your_snowflake_schema\",\n",
    "    \"sfWarehouse\": \"your_snowflake_warehouse\",\n",
    "    \"sfRole\": \"your_snowflake_role\"\n",
    "}\n",
    "\n",
    "# Iceberg configuration\n",
    "iceberg_config = {\n",
    "    \"iceberg_catalog\": \"your_iceberg_catalog\",\n",
    "    \"iceberg_namespace\": \"your_iceberg_namespace\",\n",
    "    \"iceberg_table\": \"your_iceberg_table\"\n",
    "}\n",
    "\n",
    "# Control table configuration\n",
    "control_table_config = {\n",
    "    \"control_table_name\": \"control_table\",\n",
    "    \"control_table_schema\": \"control_schema\"\n",
    "}\n",
    "\n",
    "# Processing modes\n",
    "processing_modes = {\n",
    "    \"full_load\": True,\n",
    "    \"chunk_load\": False,\n",
    "    \"partition_load\": False,\n",
    "    \"bucketing\": False,\n",
    "    \"incremental_load\": False\n",
    "}\n",
    "\n",
    "# Logging levels\n",
    "logging_levels = {\n",
    "    \"info\": logging.INFO,\n",
    "    \"debug\": logging.DEBUG,\n",
    "    \"error\": logging.ERROR\n",
    "}\n",
    "\n",
    "# Set logging level\n",
    "logger.setLevel(logging_levels[\"info\"])\n",
    "\n",
    "# Example of how to use configurations\n",
    "logger.info(\"Snowflake configuration: %s\", snowflake_config)\n",
    "logger.info(\"Iceberg configuration: %s\", iceberg_config)\n",
    "logger.info(\"Control table configuration: %s\", control_table_config)\n",
    "logger.info(\"Processing modes: %s\", processing_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema Handling\n",
    "Support replication for tables of different schemas dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Handling\n",
    "\n",
    "# Function to get schema from Snowflake table\n",
    "def get_snowflake_table_schema(table_name):\n",
    "    query = f\"SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}'\"\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=snowflake_config[\"sfUser\"],\n",
    "        password=snowflake_config[\"sfPassword\"],\n",
    "        account=snowflake_config[\"sfURL\"],\n",
    "        warehouse=snowflake_config[\"sfWarehouse\"],\n",
    "        database=snowflake_config[\"sfDatabase\"],\n",
    "        schema=snowflake_config[\"sfSchema\"],\n",
    "        role=snowflake_config[\"sfRole\"]\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    schema_info = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return schema_info\n",
    "\n",
    "# Function to create Iceberg table schema from Snowflake schema\n",
    "def create_iceberg_table_schema(snowflake_schema):\n",
    "    iceberg_schema = StructType()\n",
    "    for column in snowflake_schema:\n",
    "        column_name = column[0]\n",
    "        data_type = column[1]\n",
    "        if data_type == \"TEXT\":\n",
    "            iceberg_schema.add(StructField(column_name, StringType(), True))\n",
    "        elif data_type == \"NUMBER\":\n",
    "            iceberg_schema.add(StructField(column_name, IntegerType(), True))\n",
    "        # Add more data type mappings as needed\n",
    "    return iceberg_schema\n",
    "\n",
    "# Function to create Iceberg table if it doesn't exist\n",
    "def create_iceberg_table_if_not_exists(table_name, schema):\n",
    "    if not spark.catalog.tableExists(f\"{iceberg_config['iceberg_catalog']}.{iceberg_config['iceberg_namespace']}.{table_name}\"):\n",
    "        df = spark.createDataFrame([], schema)\n",
    "        df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"{iceberg_config['iceberg_catalog']}.{iceberg_config['iceberg_namespace']}.{table_name}\")\n",
    "        logger.info(f\"Iceberg table {table_name} created with schema: {schema}\")\n",
    "\n",
    "# Example usage\n",
    "source_table_name = \"your_source_table\"\n",
    "snowflake_schema = get_snowflake_table_schema(source_table_name)\n",
    "iceberg_schema = create_iceberg_table_schema(snowflake_schema)\n",
    "create_iceberg_table_if_not_exists(iceberg_config[\"iceberg_table\"], iceberg_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Table Creation\n",
    "If the Iceberg table doesnâ€™t exist, create it based on the source table schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Table Creation\n",
    "\n",
    "# Function to get schema from Snowflake table\n",
    "def get_snowflake_table_schema(table_name):\n",
    "    query = f\"SELECT COLUMN_NAME, DATA_TYPE FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}'\"\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=snowflake_config[\"sfUser\"],\n",
    "        password=snowflake_config[\"sfPassword\"],\n",
    "        account=snowflake_config[\"sfURL\"],\n",
    "        warehouse=snowflake_config[\"sfWarehouse\"],\n",
    "        database=snowflake_config[\"sfDatabase\"],\n",
    "        schema=snowflake_config[\"sfSchema\"],\n",
    "        role=snowflake_config[\"sfRole\"]\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    schema_info = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return schema_info\n",
    "\n",
    "# Function to create Iceberg table schema from Snowflake schema\n",
    "def create_iceberg_table_schema(snowflake_schema):\n",
    "    iceberg_schema = StructType()\n",
    "    for column in snowflake_schema:\n",
    "        column_name = column[0]\n",
    "        data_type = column[1]\n",
    "        if data_type == \"TEXT\":\n",
    "            iceberg_schema.add(StructField(column_name, StringType(), True))\n",
    "        elif data_type == \"NUMBER\":\n",
    "            iceberg_schema.add(StructField(column_name, IntegerType(), True))\n",
    "        # Add more data type mappings as needed\n",
    "    return iceberg_schema\n",
    "\n",
    "# Function to create Iceberg table if it doesn't exist\n",
    "def create_iceberg_table_if_not_exists(table_name, schema):\n",
    "    if not spark.catalog.tableExists(f\"{iceberg_config['iceberg_catalog']}.{iceberg_config['iceberg_namespace']}.{table_name}\"):\n",
    "        df = spark.createDataFrame([], schema)\n",
    "        df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"{iceberg_config['iceberg_catalog']}.{iceberg_config['iceberg_namespace']}.{table_name}\")\n",
    "        logger.info(f\"Iceberg table {table_name} created with schema: {schema}\")\n",
    "\n",
    "# Example usage\n",
    "source_table_name = \"your_source_table\"\n",
    "snowflake_schema = get_snowflake_table_schema(source_table_name)\n",
    "iceberg_schema = create_iceberg_table_schema(snowflake_schema)\n",
    "create_iceberg_table_if_not_exists(iceberg_config[\"iceberg_table\"], iceberg_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control Table Management\n",
    "Manage the control table that tracks table names and batch_sk (batch sequence key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control Table Management\n",
    "\n",
    "# Function to initialize control table if it doesn't exist\n",
    "def initialize_control_table():\n",
    "    control_table_full_name = f\"{control_table_config['control_table_schema']}.{control_table_config['control_table_name']}\"\n",
    "    if not spark.catalog.tableExists(control_table_full_name):\n",
    "        schema = StructType([\n",
    "            StructField(\"table_name\", StringType(), False),\n",
    "            StructField(\"batch_sk\", IntegerType(), False)\n",
    "        ])\n",
    "        df = spark.createDataFrame([], schema)\n",
    "        df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(control_table_full_name)\n",
    "        logger.info(f\"Control table {control_table_full_name} created.\")\n",
    "\n",
    "# Function to update control table with new batch_sk\n",
    "def update_control_table(table_name, batch_sk):\n",
    "    control_table_full_name = f\"{control_table_config['control_table_schema']}.{control_table_config['control_table_name']}\"\n",
    "    control_df = spark.table(control_table_full_name)\n",
    "    existing_entry = control_df.filter(col(\"table_name\") == table_name).collect()\n",
    "    \n",
    "    if existing_entry:\n",
    "        control_df = control_df.withColumn(\"batch_sk\", \n",
    "                                           col(\"batch_sk\").when(col(\"table_name\") == table_name, batch_sk).otherwise(col(\"batch_sk\")))\n",
    "    else:\n",
    "        new_entry = spark.createDataFrame([(table_name, batch_sk)], control_df.schema)\n",
    "        control_df = control_df.union(new_entry)\n",
    "    \n",
    "    control_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(control_table_full_name)\n",
    "    logger.info(f\"Control table {control_table_full_name} updated with table {table_name} and batch_sk {batch_sk}.\")\n",
    "\n",
    "# Function to revert control table update in case of failure\n",
    "def revert_control_table_update(table_name, previous_batch_sk):\n",
    "    update_control_table(table_name, previous_batch_sk)\n",
    "    logger.info(f\"Control table reverted to previous batch_sk {previous_batch_sk} for table {table_name}.\")\n",
    "\n",
    "# Example usage\n",
    "initialize_control_table()\n",
    "update_control_table(\"your_source_table\", 1)\n",
    "revert_control_table_update(\"your_source_table\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Control Table\n",
    "Insert an entry with batch_sk = 0 if the table is not in control_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Control Table\n",
    "\n",
    "# Function to initialize control table if it doesn't exist\n",
    "def initialize_control_table():\n",
    "    control_table_full_name = f\"{control_table_config['control_table_schema']}.{control_table_config['control_table_name']}\"\n",
    "    if not spark.catalog.tableExists(control_table_full_name):\n",
    "        schema = StructType([\n",
    "            StructField(\"table_name\", StringType(), False),\n",
    "            StructField(\"batch_sk\", IntegerType(), False)\n",
    "        ])\n",
    "        df = spark.createDataFrame([], schema)\n",
    "        df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(control_table_full_name)\n",
    "        logger.info(f\"Control table {control_table_full_name} created.\")\n",
    "\n",
    "# Function to insert an entry with batch_sk = 0 if the table is not in control_table\n",
    "def insert_initial_entry_if_not_exists(table_name):\n",
    "    control_table_full_name = f\"{control_table_config['control_table_schema']}.{control_table_config['control_table_name']}\"\n",
    "    control_df = spark.table(control_table_full_name)\n",
    "    existing_entry = control_df.filter(col(\"table_name\") == table_name).collect()\n",
    "    \n",
    "    if not existing_entry:\n",
    "        new_entry = spark.createDataFrame([(table_name, 0)], control_df.schema)\n",
    "        control_df = control_df.union(new_entry)\n",
    "        control_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(control_table_full_name)\n",
    "        logger.info(f\"Inserted initial entry for table {table_name} with batch_sk = 0 in control table.\")\n",
    "\n",
    "# Example usage\n",
    "initialize_control_table()\n",
    "insert_initial_entry_if_not_exists(\"your_source_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Control Table\n",
    "Update control_table before inserting into Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Control Table\n",
    "\n",
    "# Function to update control table before inserting into Iceberg\n",
    "def update_control_table_before_insertion(table_name, new_batch_sk):\n",
    "    control_table_full_name = f\"{control_table_config['control_table_schema']}.{control_table_config['control_table_name']}\"\n",
    "    control_df = spark.table(control_table_full_name)\n",
    "    \n",
    "    # Get the previous batch_sk for the table\n",
    "    previous_entry = control_df.filter(col(\"table_name\") == table_name).collect()\n",
    "    if previous_entry:\n",
    "        previous_batch_sk = previous_entry[0][\"batch_sk\"]\n",
    "    else:\n",
    "        previous_batch_sk = 0\n",
    "    \n",
    "    # Update the control table with the new batch_sk\n",
    "    update_control_table(table_name, new_batch_sk)\n",
    "    \n",
    "    # Return the previous batch_sk for potential rollback\n",
    "    return previous_batch_sk\n",
    "\n",
    "# Example usage\n",
    "table_name = \"your_source_table\"\n",
    "new_batch_sk = 2\n",
    "previous_batch_sk = update_control_table_before_insertion(table_name, new_batch_sk)\n",
    "logger.info(f\"Control table updated for table {table_name} with new batch_sk {new_batch_sk}. Previous batch_sk was {previous_batch_sk}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revert Control Table Update\n",
    "If the Iceberg insertion fails, revert the batch_sk update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert Control Table Update\n",
    "\n",
    "# Function to revert control table update in case of failure\n",
    "def revert_control_table_update(table_name, previous_batch_sk):\n",
    "    control_table_full_name = f\"{control_table_config['control_table_schema']}.{control_table_config['control_table_name']}\"\n",
    "    control_df = spark.table(control_table_full_name)\n",
    "    \n",
    "    # Revert the batch_sk to the previous value\n",
    "    control_df = control_df.withColumn(\"batch_sk\", \n",
    "                                       col(\"batch_sk\").when(col(\"table_name\") == table_name, previous_batch_sk).otherwise(col(\"batch_sk\")))\n",
    "    \n",
    "    control_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(control_table_full_name)\n",
    "    logger.info(f\"Control table reverted to previous batch_sk {previous_batch_sk} for table {table_name}.\")\n",
    "\n",
    "# Example usage\n",
    "table_name = \"your_source_table\"\n",
    "previous_batch_sk = 1\n",
    "revert_control_table_update(table_name, previous_batch_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Strategies\n",
    "Implement different loading strategies including Full Load, Chunk Load, Partition Load, Bucketing, and Incremental Load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Strategies\n",
    "\n",
    "# Function to perform full load\n",
    "def full_load(source_table, target_table):\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    logger.info(f\"Full load completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Function to perform chunk load\n",
    "def chunk_load(source_table, target_table, chunk_size):\n",
    "    offset = 0\n",
    "    while True:\n",
    "        query = f\"(SELECT * FROM {source_table} LIMIT {chunk_size} OFFSET {offset})\"\n",
    "        df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_config) \\\n",
    "            .option(\"query\", query) \\\n",
    "            .load()\n",
    "        if df.count() == 0:\n",
    "            break\n",
    "        df.write.format(\"iceberg\").mode(\"append\").saveAsTable(target_table)\n",
    "        offset += chunk_size\n",
    "        logger.info(f\"Chunk load completed for offset {offset} for table {source_table} into {target_table}\")\n",
    "\n",
    "# Function to perform partition load\n",
    "def partition_load(source_table, target_table, partition_column):\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    df.write.partitionBy(partition_column).format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    logger.info(f\"Partition load completed for table {source_table} into {target_table} by partition column {partition_column}\")\n",
    "\n",
    "# Function to perform bucketing\n",
    "def bucketing_load(source_table, target_table, bucket_column, num_buckets):\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    df.write.bucketBy(num_buckets, bucket_column).sortBy(bucket_column).format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    logger.info(f\"Bucketing load completed for table {source_table} into {target_table} by bucket column {bucket_column} with {num_buckets} buckets\")\n",
    "\n",
    "# Function to perform incremental load\n",
    "def incremental_load(source_table, target_table, last_batch_sk):\n",
    "    query = f\"(SELECT * FROM {source_table} WHERE batch_sk > {last_batch_sk})\"\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    df.write.format(\"iceberg\").mode(\"append\").saveAsTable(target_table)\n",
    "    logger.info(f\"Incremental load completed for table {source_table} into {target_table} with batch_sk > {last_batch_sk}\")\n",
    "\n",
    "# Example usage\n",
    "if processing_modes[\"full_load\"]:\n",
    "    full_load(\"your_source_table\", iceberg_config[\"iceberg_table\"])\n",
    "elif processing_modes[\"chunk_load\"]:\n",
    "    chunk_load(\"your_source_table\", iceberg_config[\"iceberg_table\"], chunk_size=1000)\n",
    "elif processing_modes[\"partition_load\"]:\n",
    "    partition_load(\"your_source_table\", iceberg_config[\"iceberg_table\"], partition_column=\"your_partition_column\")\n",
    "elif processing_modes[\"bucketing\"]:\n",
    "    bucketing_load(\"your_source_table\", iceberg_config[\"iceberg_table\"], bucket_column=\"your_bucket_column\", num_buckets=10)\n",
    "elif processing_modes[\"incremental_load\"]:\n",
    "    last_batch_sk = 0  # This should be fetched from the control table\n",
    "    incremental_load(\"your_source_table\", iceberg_config[\"iceberg_table\"], last_batch_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Load\n",
    "Load the entire dataset from the source table to the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Load\n",
    "\n",
    "# Function to perform full load\n",
    "def full_load(source_table, target_table):\n",
    "    # Read the entire dataset from the Snowflake source table\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    \n",
    "    # Write the dataset to the Iceberg target table\n",
    "    df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of the full load\n",
    "    logger.info(f\"Full load completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "if processing_modes[\"full_load\"]:\n",
    "    full_load(\"your_source_table\", iceberg_config[\"iceberg_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk Load\n",
    "Load data in chunks from the source table to the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk Load\n",
    "\n",
    "# Function to perform chunk load\n",
    "def chunk_load(source_table, target_table, chunk_size):\n",
    "    offset = 0\n",
    "    while True:\n",
    "        query = f\"(SELECT * FROM {source_table} LIMIT {chunk_size} OFFSET {offset})\"\n",
    "        df = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**snowflake_config) \\\n",
    "            .option(\"query\", query) \\\n",
    "            .load()\n",
    "        if df.count() == 0:\n",
    "            break\n",
    "        df.write.format(\"iceberg\").mode(\"append\").saveAsTable(target_table)\n",
    "        offset += chunk_size\n",
    "        logger.info(f\"Chunk load completed for offset {offset} for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "if processing_modes[\"chunk_load\"]:\n",
    "    chunk_load(\"your_source_table\", iceberg_config[\"iceberg_table\"], chunk_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition Load\n",
    "Load data based on partitions from the source table to the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Load\n",
    "\n",
    "# Function to perform partition load\n",
    "def partition_load(source_table, target_table, partition_column):\n",
    "    # Read the dataset from the Snowflake source table\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    \n",
    "    # Write the dataset to the Iceberg target table partitioned by the specified column\n",
    "    df.write.partitionBy(partition_column).format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of the partition load\n",
    "    logger.info(f\"Partition load completed for table {source_table} into {target_table} by partition column {partition_column}\")\n",
    "\n",
    "# Example usage\n",
    "if processing_modes[\"partition_load\"]:\n",
    "    partition_load(\"your_source_table\", iceberg_config[\"iceberg_table\"], partition_column=\"your_partition_column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing\n",
    "Load data using bucketing strategy from the source table to the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketing\n",
    "\n",
    "# Function to perform bucketing load\n",
    "def bucketing_load(source_table, target_table, bucket_column, num_buckets):\n",
    "    # Read the dataset from the Snowflake source table\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    \n",
    "    # Write the dataset to the Iceberg target table using bucketing\n",
    "    df.write.bucketBy(num_buckets, bucket_column).sortBy(bucket_column).format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of the bucketing load\n",
    "    logger.info(f\"Bucketing load completed for table {source_table} into {target_table} by bucket column {bucket_column} with {num_buckets} buckets\")\n",
    "\n",
    "# Example usage\n",
    "if processing_modes[\"bucketing\"]:\n",
    "    bucketing_load(\"your_source_table\", iceberg_config[\"iceberg_table\"], bucket_column=\"your_bucket_column\", num_buckets=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Load\n",
    "Load only the new or updated data from the source table to the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental Load\n",
    "\n",
    "# Function to perform incremental load\n",
    "def incremental_load(source_table, target_table, last_batch_sk):\n",
    "    # Query to fetch only new or updated data from the Snowflake source table\n",
    "    query = f\"(SELECT * FROM {source_table} WHERE batch_sk > {last_batch_sk})\"\n",
    "    \n",
    "    # Read the incremental data from the Snowflake source table\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    \n",
    "    # Write the incremental data to the Iceberg target table\n",
    "    df.write.format(\"iceberg\").mode(\"append\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of the incremental load\n",
    "    logger.info(f\"Incremental load completed for table {source_table} into {target_table} with batch_sk > {last_batch_sk}\")\n",
    "\n",
    "# Example usage\n",
    "if processing_modes[\"incremental_load\"]:\n",
    "    # Fetch the last batch_sk from the control table\n",
    "    control_table_full_name = f\"{control_table_config['control_table_schema']}.{control_table_config['control_table_name']}\"\n",
    "    control_df = spark.table(control_table_full_name)\n",
    "    last_batch_sk = control_df.filter(col(\"table_name\") == source_table_name).select(\"batch_sk\").collect()[0][\"batch_sk\"]\n",
    "    \n",
    "    # Perform incremental load\n",
    "    incremental_load(source_table_name, iceberg_config[\"iceberg_table\"], last_batch_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Setup\n",
    "Implement logging at each step to track progress and potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Setup\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set logging level\n",
    "logger.setLevel(logging_levels[\"info\"])\n",
    "\n",
    "# Example of how to use logging\n",
    "logger.info(\"Logging setup complete.\")\n",
    "logger.info(\"Snowflake configuration: %s\", snowflake_config)\n",
    "logger.info(\"Iceberg configuration: %s\", iceberg_config)\n",
    "logger.info(\"Control table configuration: %s\", control_table_config)\n",
    "logger.info(\"Processing modes: %s\", processing_modes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Strategy\n",
    "Provide configurable options for Append, Overwrite, and Merge (Upsert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Strategy\n",
    "\n",
    "# Function to perform append operation\n",
    "def append_data(source_table, target_table):\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    df.write.format(\"iceberg\").mode(\"append\").saveAsTable(target_table)\n",
    "    logger.info(f\"Append operation completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Function to perform overwrite operation\n",
    "def overwrite_data(source_table, target_table):\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    logger.info(f\"Overwrite operation completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Function to perform merge (upsert) operation\n",
    "def merge_data(source_table, target_table, primary_key):\n",
    "    source_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    \n",
    "    target_df = spark.table(target_table)\n",
    "    \n",
    "    # Perform the merge (upsert) operation\n",
    "    merged_df = source_df.alias(\"source\").join(target_df.alias(\"target\"), primary_key, \"outer\") \\\n",
    "        .select(\n",
    "            *[col(\"source.\" + col_name).alias(col_name) for col_name in source_df.columns],\n",
    "            *[col(\"target.\" + col_name).alias(col_name) for col_name in target_df.columns if col_name not in source_df.columns]\n",
    "        )\n",
    "    \n",
    "    merged_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    logger.info(f\"Merge (upsert) operation completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "merge_strategy = \"append\"  # Change this to \"overwrite\" or \"merge\" as needed\n",
    "primary_key = \"id\"  # Change this to the actual primary key column\n",
    "\n",
    "if merge_strategy == \"append\":\n",
    "    append_data(\"your_source_table\", iceberg_config[\"iceberg_table\"])\n",
    "elif merge_strategy == \"overwrite\":\n",
    "    overwrite_data(\"your_source_table\", iceberg_config[\"iceberg_table\"])\n",
    "elif merge_strategy == \"merge\":\n",
    "    merge_data(\"your_source_table\", iceberg_config[\"iceberg_table\"], primary_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append\n",
    "Append new data to the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append\n",
    "\n",
    "# Function to append new data to the target table\n",
    "def append_data(source_table, target_table):\n",
    "    # Read new data from the Snowflake source table\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    \n",
    "    # Append the new data to the Iceberg target table\n",
    "    df.write.format(\"iceberg\").mode(\"append\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of the append operation\n",
    "    logger.info(f\"Append operation completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "append_data(\"your_source_table\", iceberg_config[\"iceberg_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite\n",
    "Overwrite the existing data in the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite\n",
    "\n",
    "# Function to overwrite the existing data in the target table\n",
    "def overwrite_data(source_table, target_table):\n",
    "    # Read the entire dataset from the Snowflake source table\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    \n",
    "    # Overwrite the existing data in the Iceberg target table\n",
    "    df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of the overwrite operation\n",
    "    logger.info(f\"Overwrite operation completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "overwrite_data(\"your_source_table\", iceberg_config[\"iceberg_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge (Upsert)\n",
    "Merge new data with existing data in the target table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge (Upsert)\n",
    "\n",
    "# Function to perform merge (upsert) operation\n",
    "def merge_data(source_table, target_table, primary_key):\n",
    "    # Read new data from the Snowflake source table\n",
    "    source_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"dbtable\", source_table) \\\n",
    "        .load()\n",
    "    \n",
    "    # Read existing data from the Iceberg target table\n",
    "    target_df = spark.table(target_table)\n",
    "    \n",
    "    # Perform the merge (upsert) operation\n",
    "    merged_df = source_df.alias(\"source\").join(target_df.alias(\"target\"), primary_key, \"outer\") \\\n",
    "        .select(\n",
    "            *[col(\"source.\" + col_name).alias(col_name) for col_name in source_df.columns],\n",
    "            *[col(\"target.\" + col_name).alias(col_name) for col_name in target_df.columns if col_name not in source_df.columns]\n",
    "        )\n",
    "    \n",
    "    # Write the merged data back to the Iceberg target table\n",
    "    merged_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of the merge (upsert) operation\n",
    "    logger.info(f\"Merge (upsert) operation completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "primary_key = \"id\"  # Change this to the actual primary key column\n",
    "merge_data(\"your_source_table\", iceberg_config[\"iceberg_table\"], primary_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Change Handling\n",
    "Capture INSERT, UPDATE, and DELETE operations from Snowflake and reflect these changes in Iceberg efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Change Handling\n",
    "\n",
    "# Function to capture INSERT operations from Snowflake and reflect in Iceberg\n",
    "def handle_insert_operations(source_table, target_table):\n",
    "    query = f\"(SELECT * FROM {source_table} WHERE op_type = 'INSERT')\"\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    df.write.format(\"iceberg\").mode(\"append\").saveAsTable(target_table)\n",
    "    logger.info(f\"INSERT operations handled for table {source_table} into {target_table}\")\n",
    "\n",
    "# Function to capture UPDATE operations from Snowflake and reflect in Iceberg\n",
    "def handle_update_operations(source_table, target_table, primary_key):\n",
    "    query = f\"(SELECT * FROM {source_table} WHERE op_type = 'UPDATE')\"\n",
    "    source_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    \n",
    "    target_df = spark.table(target_table)\n",
    "    \n",
    "    merged_df = source_df.alias(\"source\").join(target_df.alias(\"target\"), primary_key, \"left\") \\\n",
    "        .select(\n",
    "            *[col(\"source.\" + col_name).alias(col_name) for col_name in source_df.columns]\n",
    "        )\n",
    "    \n",
    "    merged_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    logger.info(f\"UPDATE operations handled for table {source_table} into {target_table}\")\n",
    "\n",
    "# Function to capture DELETE operations from Snowflake and reflect in Iceberg\n",
    "def handle_delete_operations(source_table, target_table, primary_key):\n",
    "    query = f\"(SELECT * FROM {source_table} WHERE op_type = 'DELETE')\"\n",
    "    delete_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    \n",
    "    target_df = spark.table(target_table)\n",
    "    \n",
    "    remaining_df = target_df.join(delete_df, primary_key, \"left_anti\")\n",
    "    \n",
    "    remaining_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    logger.info(f\"DELETE operations handled for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "handle_insert_operations(\"your_source_table\", iceberg_config[\"iceberg_table\"])\n",
    "handle_update_operations(\"your_source_table\", iceberg_config[\"iceberg_table\"], primary_key=\"id\")\n",
    "handle_delete_operations(\"your_source_table\", iceberg_config[\"iceberg_table\"], primary_key=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture INSERT Operations\n",
    "Capture and handle INSERT operations from the source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture INSERT Operations\n",
    "\n",
    "# Function to capture and handle INSERT operations from the source table\n",
    "def capture_insert_operations(source_table, target_table):\n",
    "    # Query to fetch INSERT operations from the Snowflake source table\n",
    "    query = f\"(SELECT * FROM {source_table} WHERE op_type = 'INSERT')\"\n",
    "    \n",
    "    # Read the INSERT operations from the Snowflake source table\n",
    "    df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    \n",
    "    # Append the INSERT operations to the Iceberg target table\n",
    "    df.write.format(\"iceberg\").mode(\"append\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of handling INSERT operations\n",
    "    logger.info(f\"INSERT operations captured and handled for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "capture_insert_operations(\"your_source_table\", iceberg_config[\"iceberg_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture UPDATE Operations\n",
    "Capture and handle UPDATE operations from the source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture UPDATE Operations\n",
    "\n",
    "# Function to capture and handle UPDATE operations from the source table\n",
    "def capture_update_operations(source_table, target_table, primary_key):\n",
    "    # Query to fetch UPDATE operations from the Snowflake source table\n",
    "    query = f\"(SELECT * FROM {source_table} WHERE op_type = 'UPDATE')\"\n",
    "    \n",
    "    # Read the UPDATE operations from the Snowflake source table\n",
    "    source_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    \n",
    "    # Read the existing data from the Iceberg target table\n",
    "    target_df = spark.table(target_table)\n",
    "    \n",
    "    # Perform the merge (upsert) operation\n",
    "    merged_df = source_df.alias(\"source\").join(target_df.alias(\"target\"), primary_key, \"left\") \\\n",
    "        .select(\n",
    "            *[col(\"source.\" + col_name).alias(col_name) for col_name in source_df.columns]\n",
    "        )\n",
    "    \n",
    "    # Write the merged data back to the Iceberg target table\n",
    "    merged_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of handling UPDATE operations\n",
    "    logger.info(f\"UPDATE operations captured and handled for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "capture_update_operations(\"your_source_table\", iceberg_config[\"iceberg_table\"], primary_key=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture DELETE Operations\n",
    "Capture and handle DELETE operations from the source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture DELETE Operations\n",
    "\n",
    "# Function to capture and handle DELETE operations from the source table\n",
    "def capture_delete_operations(source_table, target_table, primary_key):\n",
    "    # Query to fetch DELETE operations from the Snowflake source table\n",
    "    query = f\"(SELECT * FROM {source_table} WHERE op_type = 'DELETE')\"\n",
    "    \n",
    "    # Read the DELETE operations from the Snowflake source table\n",
    "    delete_df = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**snowflake_config) \\\n",
    "        .option(\"query\", query) \\\n",
    "        .load()\n",
    "    \n",
    "    # Read the existing data from the Iceberg target table\n",
    "    target_df = spark.table(target_table)\n",
    "    \n",
    "    # Perform the anti join to remove the deleted records\n",
    "    remaining_df = target_df.join(delete_df, primary_key, \"left_anti\")\n",
    "    \n",
    "    # Write the remaining data back to the Iceberg target table\n",
    "    remaining_df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "    \n",
    "    # Log the completion of handling DELETE operations\n",
    "    logger.info(f\"DELETE operations captured and handled for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "capture_delete_operations(\"your_source_table\", iceberg_config[\"iceberg_table\"], primary_key=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Logic\n",
    "Define the workflow logic for initial table creation & data insertion and existing table with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow Logic\n",
    "\n",
    "# Function to handle initial table creation and data insertion\n",
    "def initial_table_creation_and_insertion(source_table, target_table):\n",
    "    # Get the schema from the Snowflake source table\n",
    "    snowflake_schema = get_snowflake_table_schema(source_table)\n",
    "    \n",
    "    # Create the Iceberg table schema from the Snowflake schema\n",
    "    iceberg_schema = create_iceberg_table_schema(snowflake_schema)\n",
    "    \n",
    "    # Create the Iceberg table if it doesn't exist\n",
    "    create_iceberg_table_if_not_exists(target_table, iceberg_schema)\n",
    "    \n",
    "    # Insert initial entry in control table if it doesn't exist\n",
    "    insert_initial_entry_if_not_exists(source_table)\n",
    "    \n",
    "    # Perform full load from Snowflake to Iceberg\n",
    "    full_load(source_table, target_table)\n",
    "    \n",
    "    # Update control table with new batch_sk\n",
    "    new_batch_sk = 1  # This should be fetched from the source table\n",
    "    update_control_table(source_table, new_batch_sk)\n",
    "    \n",
    "    logger.info(f\"Initial table creation and data insertion completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Function to handle existing table with new data\n",
    "def existing_table_with_new_data(source_table, target_table):\n",
    "    # Fetch the last batch_sk from the control table\n",
    "    control_table_full_name = f\"{control_table_config['control_table_schema']}.{control_table_config['control_table_name']}\"\n",
    "    control_df = spark.table(control_table_full_name)\n",
    "    last_batch_sk = control_df.filter(col(\"table_name\") == source_table).select(\"batch_sk\").collect()[0][\"batch_sk\"]\n",
    "    \n",
    "    # Perform incremental load from Snowflake to Iceberg\n",
    "    incremental_load(source_table, target_table, last_batch_sk)\n",
    "    \n",
    "    # Update control table with new batch_sk\n",
    "    new_batch_sk = last_batch_sk + 1  # This should be fetched from the source table\n",
    "    previous_batch_sk = update_control_table_before_insertion(source_table, new_batch_sk)\n",
    "    \n",
    "    # If the Iceberg insertion fails, revert the batch_sk update\n",
    "    try:\n",
    "        incremental_load(source_table, target_table, last_batch_sk)\n",
    "    except Exception as e:\n",
    "        revert_control_table_update(source_table, previous_batch_sk)\n",
    "        logger.error(f\"Error during incremental load: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    logger.info(f\"Existing table with new data handled for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "initial_table_creation_and_insertion(\"your_source_table\", iceberg_config[\"iceberg_table\"])\n",
    "existing_table_with_new_data(\"your_source_table\", iceberg_config[\"iceberg_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Table Creation & Data Insertion\n",
    "Handle the case where the table is newly created and new data is inserted with an increasing batch_sk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Table Creation & Data Insertion\n",
    "\n",
    "# Function to handle initial table creation and data insertion\n",
    "def initial_table_creation_and_insertion(source_table, target_table):\n",
    "    # Get the schema from the Snowflake source table\n",
    "    snowflake_schema = get_snowflake_table_schema(source_table)\n",
    "    \n",
    "    # Create the Iceberg table schema from the Snowflake schema\n",
    "    iceberg_schema = create_iceberg_table_schema(snowflake_schema)\n",
    "    \n",
    "    # Create the Iceberg table if it doesn't exist\n",
    "    create_iceberg_table_if_not_exists(target_table, iceberg_schema)\n",
    "    \n",
    "    # Insert initial entry in control table if it doesn't exist\n",
    "    insert_initial_entry_if_not_exists(source_table)\n",
    "    \n",
    "    # Perform full load from Snowflake to Iceberg\n",
    "    full_load(source_table, target_table)\n",
    "    \n",
    "    # Update control table with new batch_sk\n",
    "    new_batch_sk = 1  # This should be fetched from the source table\n",
    "    update_control_table(source_table, new_batch_sk)\n",
    "    \n",
    "    logger.info(f\"Initial table creation and data insertion completed for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "initial_table_creation_and_insertion(\"your_source_table\", iceberg_config[\"iceberg_table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existing Table with New Data\n",
    "Handle the case where new data is inserted into an existing table, updating batch_sk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing Table with New Data\n",
    "\n",
    "# Function to handle existing table with new data\n",
    "def existing_table_with_new_data(source_table, target_table):\n",
    "    # Fetch the last batch_sk from the control table\n",
    "    control_table_full_name = f\"{control_table_config['control_table_schema']}.{control_table_config['control_table_name']}\"\n",
    "    control_df = spark.table(control_table_full_name)\n",
    "    last_batch_sk = control_df.filter(col(\"table_name\") == source_table).select(\"batch_sk\").collect()[0][\"batch_sk\"]\n",
    "    \n",
    "    # Update control table with new batch_sk\n",
    "    new_batch_sk = last_batch_sk + 1  # This should be fetched from the source table\n",
    "    previous_batch_sk = update_control_table_before_insertion(source_table, new_batch_sk)\n",
    "    \n",
    "    # Perform incremental load from Snowflake to Iceberg\n",
    "    try:\n",
    "        incremental_load(source_table, target_table, last_batch_sk)\n",
    "    except Exception as e:\n",
    "        revert_control_table_update(source_table, previous_batch_sk)\n",
    "        logger.error(f\"Error during incremental load: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    logger.info(f\"Existing table with new data handled for table {source_table} into {target_table}\")\n",
    "\n",
    "# Example usage\n",
    "existing_table_with_new_data(\"your_source_table\", iceberg_config[\"iceberg_table\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
